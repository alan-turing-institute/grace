{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure GRACE performance on identified objects:\n",
    "\n",
    "All metrics here are **exact**, i.e measure similarity between two graphs formed of identical node count which have the same positions. The only thing that differs between the graphs is the edges and object membership / individual identities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grace.io.image_dataset import ImageGraphDataset\n",
    "from grace.models.optimiser import optimise_graph\n",
    "from grace.simulator.simulate_graph import random_graph\n",
    "\n",
    "from grace.evaluation.metrics_objects import (\n",
    "    ExactMetricsComputer,\n",
    "    ApproxMetricsComputer,\n",
    ")\n",
    "\n",
    "from grace.evaluation.process import (\n",
    "    generate_ground_truth_graph,\n",
    "    update_graph_with_dummy_predictions,\n",
    "    add_and_remove_random_edges,\n",
    ")\n",
    "from grace.evaluation.visualisation import (\n",
    "    plot_simple_graph,\n",
    "    plot_connected_components,\n",
    ")\n",
    "from grace.evaluation.annotate import (\n",
    "    draw_annotation_mask_from_ground_truth_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *EXACT* metrics on graph:\n",
    "\n",
    "### Generate a random graph, update dummy labels & generate GT graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTIFS = [\"line\", \"curve\"]\n",
    "N_MOTIFS = 2\n",
    "DENSITY = 0.01\n",
    "\n",
    "def generate_random_optimised_and_GT_graphs():\n",
    "    G = random_graph(\n",
    "        n_motifs=N_MOTIFS, \n",
    "        density=DENSITY, \n",
    "        motifs=MOTIFS\n",
    "    )\n",
    "    node_conf, edge_conf = 0.5, 0.1\n",
    "    update_graph_with_dummy_predictions(\n",
    "        G, \n",
    "        node_uncertainty=node_conf, \n",
    "        edge_uncertainty=edge_conf\n",
    "    )\n",
    "    true_graph = generate_ground_truth_graph(G)\n",
    "    pred_graph = optimise_graph(G)\n",
    "\n",
    "    return G, pred_graph, true_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep making graphs until the prediction and GT come out different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize = True\n",
    "counter = 0\n",
    "\n",
    "while synthesize:\n",
    "    counter += 1\n",
    "    print (f\"Synthesizing imperfectly optimised graph: iter = {counter}\")\n",
    "\n",
    "    G, pred_graph, true_graph = generate_random_optimised_and_GT_graphs()\n",
    "    node_con = pred_graph.number_of_nodes() == true_graph.number_of_nodes()\n",
    "    edge_con = pred_graph.number_of_edges() != true_graph.number_of_edges()\n",
    "    \n",
    "    if node_con and edge_con:\n",
    "        synthesize = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the resulting graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 5\n",
    "_, axes = plt.subplots(1, 3, figsize=(shape*3, shape*1))\n",
    "\n",
    "plot_simple_graph(G, title=f\"Random graph with {N_MOTIFS} {MOTIFS}\", ax=axes[0])\n",
    "plot_connected_components(true_graph, title=f\"Ground truth graph\", ax=axes[1])\n",
    "plot_connected_components(pred_graph, title=f\"Dummy optimised graph\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the *exact* metrics on graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMC = ExactMetricsComputer(G=G, pred_optimised_graph=pred_graph, true_annotated_graph=true_graph)\n",
    "results = EMC.metrics()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMC.visualise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Approx* metrics on annotation masks:\n",
    "\n",
    "### Open an annotated image & visualise the annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_path = \"/Users/kulicna/Desktop/dataset/shape_stars/infer\"\n",
    "dataset = ImageGraphDataset(\n",
    "    image_dir=grace_path, \n",
    "    grace_dir=grace_path, \n",
    "    keep_node_unknown_labels=False,\n",
    "    keep_edge_unknown_labels=False,\n",
    "    verbose=True\n",
    ")\n",
    "image, graph_data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the GT graph & both hand- and auto-annotated masks:\n",
    "\n",
    "To demonstrate that the metrics evaluation is really working, add & remove a few random edges just to mess up with the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graph_data[\"graph\"]\n",
    "update_graph_with_dummy_predictions(G)\n",
    "\n",
    "true_graph = generate_ground_truth_graph(G)\n",
    "true_anno = graph_data[\"annotation\"]\n",
    "\n",
    "pred_graph = add_and_remove_random_edges(true_graph, G, 3, 5)\n",
    "pred_anno = draw_annotation_mask_from_ground_truth_graph(\n",
    "    pred_graph, \n",
    "    shape=image.shape, \n",
    "    brush_size=75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise together with the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_masks(image_dict: dict[str, np.ndarray], figsize : tuple[int, int] = (15, 5)):\n",
    "    _, axes = plt.subplots(1, len(image_dict), figsize=figsize)\n",
    "\n",
    "    for i, (title, image) in enumerate(image_dict.items()):\n",
    "        im = axes[i].imshow(image, cmap=\"binary_r\")\n",
    "        axes[i].set_title(title)\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.045)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = {\n",
    "    \"Original Image\" : image,\n",
    "    \"Hand Annotated\" : true_anno,\n",
    "    \"Auto Annotated\" : pred_anno,\n",
    "}\n",
    "plot_image_with_masks(image_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the *approx* metrics on masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMC = ApproxMetricsComputer(G, pred_anno, true_anno)\n",
    "results = AMC.metrics()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMC.visualise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
